{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.linalg\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement in Python a first version of your own linear support vector machine with the squared hinge loss. Recall from the lectures that the linear support vector machine with the squared hinge loss writes as\n",
    "$$min_{\\beta \\in R^d} F(\\beta) := \\frac{1}{n}\\sum_{i=1}^n (max(0, 1 - y_i x_i^T \\beta))^2 + \\lambda||\\beta||_2^2$$\n",
    "\n",
    "You know now by heart the fast gradient algorithm, so no need to recall it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Compute the gradient $\\bigtriangledown F(\\beta)$ of F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\bigtriangledown F(\\beta) = \\frac{1}{n} \\sum_{i=1}^n  - 2 y_i x_i (max(0, 1 - y_ix_i^T\\beta)) + 2\\lambda\\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Consider the Spam dataset from The Elements of Statistical Learning. Standardize the data, if you have not done so already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam = pd.read_table('https://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/spam.data', sep=' ', header=None)\n",
    "test_indicator = pd.read_table('https://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/spam.traintest', sep=' ',\n",
    "                               header=None)\n",
    "\n",
    "x = np.asarray(spam)[:, 0:-1]\n",
    "y = np.asarray(spam)[:, -1]*2 - 1  # Convert to +/- 1\n",
    "test_indicator = np.array(test_indicator).T[0]\n",
    "\n",
    "\n",
    "x_train = x[test_indicator == 0, :]\n",
    "x_test = x[test_indicator == 1, :]\n",
    "y_train = y[test_indicator == 0]\n",
    "y_test = y[test_indicator == 1]\n",
    "\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n",
    "d = np.size(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3065, 57)\n",
      "(3065,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Write a function mylinearsvm that implements the fast gradient algorithm to train the linear support vector machine with the squared hinge loss. The function takes as input the initial step-size value for the backtracking rule and a maximum number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$min_{\\beta \\in R^d} F(\\beta) := \\frac{1}{n}\\sum_{i=1}^n (max(0, 1 - y_i x_i^T \\beta))^2 + \\lambda||\\beta||_2^2$$\n",
    "\n",
    "$$\\bigtriangledown F(\\beta) = \\frac{1}{n} \\sum_{i=1}^n  - 2 y_i x_i (max(0, 1 - y_ix_i^T\\beta)) + 2\\lambda\\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(beta, lambduh, x=x_train, y=y_train):\n",
    "    return 1/len(y)*np.sum(np.maximum(0, (1 - y*x.dot(beta)))**2) + lambduh * np.linalg.norm(beta)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_init = np.zeros(d)\n",
    "lambduh = 1\n",
    "objective(beta_init,lambduh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computegrad(beta, lambduh, x=x_train, y=y_train):\n",
    "    yx = y[:, None]*x\n",
    "    grad = 1/len(y)*np.sum(- 2*yx*np.maximum(0, (1 - yx.dot(beta[:, np.newaxis]))), axis=0) + 2*lambduh*beta\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22408819,  0.08028865, -0.42114039, -0.11270631, -0.44798231,\n",
       "       -0.47515217, -0.6695687 , -0.40930196, -0.44519011, -0.25808667,\n",
       "       -0.48295143, -0.01527649, -0.24073459, -0.12683508, -0.3867112 ,\n",
       "       -0.66190166, -0.53327988, -0.40871324, -0.54373398, -0.39477808,\n",
       "       -0.78523626, -0.19987572, -0.6594691 , -0.46372121,  0.51791743,\n",
       "        0.46210432,  0.36301945,  0.35862477,  0.26339397,  0.34820032,\n",
       "        0.27429528,  0.23597337,  0.22237037,  0.23602568,  0.2695741 ,\n",
       "        0.28227972,  0.35221735,  0.06570354,  0.26913151,  0.12257789,\n",
       "        0.18963924,  0.26427622,  0.26898028,  0.18567678,  0.27443554,\n",
       "        0.29737846,  0.08803932,  0.169576  ,  0.11007046,  0.16268013,\n",
       "        0.13099409, -0.4238648 , -0.64031183, -0.12885561, -0.22004094,\n",
       "       -0.37744796, -0.46470604])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_init = np.zeros(d)\n",
    "lambduh = 1\n",
    "computegrad(beta_init,lambduh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bt_line_search(beta, lambduh, eta=1, alpha=0.5, betaparam=0.8,\n",
    "                   maxiter=100, x=x_train, y=y_train):\n",
    "    grad_beta = computegrad(beta, lambduh, x=x, y=y)\n",
    "    norm_grad_beta = np.linalg.norm(grad_beta)\n",
    "    found_eta = 0\n",
    "    iter = 0\n",
    "    while found_eta == 0 and iter < maxiter:\n",
    "        if objective(beta - eta * grad_beta, lambduh, x=x, y=y) < objective(beta, lambduh, x=x, y=y) \\\n",
    "                - alpha * eta * norm_grad_beta ** 2:\n",
    "            found_eta = 1\n",
    "        elif iter == maxiter - 1:\n",
    "            print('Warning: Max number of iterations of backtracking line search reached')\n",
    "        else:\n",
    "            eta *= betaparam\n",
    "            iter += 1\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08589934592000005"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt_line_search(beta_init, lambduh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The function takes as input the initial step-size value for the backtracking rule and a maximum number of iterations.\n",
    "def mylinearsvm(eta_init, max_iter, lambduh):\n",
    "    d = np.size(x_train, 1)\n",
    "    beta = np.zeros(d)\n",
    "    theta = np.zeros(d)\n",
    "    x=x_train\n",
    "    y=y_train\n",
    "    \n",
    "    grad_theta = computegrad(theta, lambduh, x=x, y=y)\n",
    "    beta_vals = beta\n",
    "    theta_vals = theta\n",
    "    iter = 0\n",
    "    while iter < max_iter:\n",
    "        eta = bt_line_search(theta, lambduh, eta=eta_init, x=x, y=y)\n",
    "        beta_new = theta - eta*grad_theta\n",
    "        theta = beta_new + iter/(iter+3)*(beta_new-beta)\n",
    "        beta_vals = np.vstack((beta_vals, beta_new))\n",
    "        theta_vals = np.vstack((theta_vals, theta))\n",
    "        grad_theta = computegrad(theta, lambduh, x=x, y=y)\n",
    "        beta = beta_new\n",
    "        iter += 1\n",
    "        \n",
    "    return beta_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.02298016, -0.00823357,  0.04318778, ...,  0.02256511,\n",
       "         0.03870714,  0.04765542],\n",
       "       [ 0.01590594, -0.0132766 ,  0.04439642, ...,  0.02530926,\n",
       "         0.04376547,  0.05366989],\n",
       "       ..., \n",
       "       [ 0.00660427, -0.01658113,  0.03896991, ...,  0.0266002 ,\n",
       "         0.04828979,  0.05653306],\n",
       "       [ 0.00660427, -0.01658113,  0.03896991, ...,  0.02660021,\n",
       "         0.04828978,  0.05653305],\n",
       "       [ 0.00660427, -0.01658113,  0.03896991, ...,  0.02660021,\n",
       "         0.04828978,  0.05653305]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_init = 1/(scipy.linalg.eigh(1/len(y_train)*x_train.T.dot(x_train), eigvals=(d-1, d-1), eigvals_only=True)[0]+lambduh)\n",
    "max_iter = 50\n",
    "lambduh = 1\n",
    "mylinearsvm(eta_init, max_iter,lambduh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Train your linear support vector machine with the squared hinge loss on the the Spam dataset for the $\\lambda$ = 1. Report your misclassification error for this value of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eta_init = 1/(scipy.linalg.eigh(1/len(y_train)*x_train.T.dot(x_train), eigvals=(d-1, d-1), eigvals_only=True)[0]+lambduh)\n",
    "max_iter = 50\n",
    "lambduh = 1\n",
    "betas_fg= mylinearsvm(eta_init, max_iter, lambduh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_misclassification_error(beta_opt, x, y):\n",
    "    y_pred = 1/(1+np.exp(-x.dot(beta_opt))) > 0.5\n",
    "    y_pred = y_pred*2 - 1 \n",
    "    return np.mean(y_pred != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The misclassification error for lambda = 1 is 9.50520833333 %\n"
     ]
    }
   ],
   "source": [
    "beta_opt = betas_fg[-1]\n",
    "mc_error = compute_misclassification_error(beta_opt, x_test, y_test)\n",
    "print(\"The misclassification error for lambda = 1 is\", + mc_error*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Run cross-validation to find the optimal value of $\\lambda$. Report your misclassification error\n",
    "for that value of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification error from my linear SVM: [ 0.08463542  0.08528646  0.08463542  0.08919271  0.08854167  0.08789062\n",
      "  0.08789062  0.08919271  0.08984375  0.09114583  0.09179688  0.09114583\n",
      "  0.08984375  0.08919271  0.08854167  0.08919271  0.09114583  0.09179688\n",
      "  0.09114583  0.09244792  0.09505208]\n",
      "Smallest misclassification error value: 0.0846354166667 at lambda = 0.01\n"
     ]
    }
   ],
   "source": [
    "lambdas = [10**i for i in np.arange(-2, 0.1, 0.1)]\n",
    "misclass_error = np.zeros_like(lambdas)\n",
    "max_iter = 50\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    lambduh = lambdas[i]\n",
    "    beta_init = np.zeros(d)\n",
    "    eta_init = 1/(scipy.linalg.eigh(1/len(y_train)*x_train.T.dot(x_train), eigvals=(d-1, d-1), eigvals_only=True)[0]+lambduh)\n",
    "    betas_svm = mylinearsvm(eta_init, max_iter,lambduh)\n",
    "    misclass_error[i] = compute_misclassification_error(betas_svm[-1], x_test, y_test)\n",
    "    \n",
    "print('Misclassification error from my linear SVM:', misclass_error)\n",
    "\n",
    "print('Smallest misclassification error value:', min(misclass_error), 'at lambda =', lambdas[np.argmin(misclass_error)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
